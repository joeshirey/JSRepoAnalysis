Your task is to meticulously evaluate the provided code sample for the specified **{{LANGUAGE}}** based on the criteria outlined below. Your entire output MUST be a single, valid JSON object, with no introductory text or explanations outside of the JSON structure. It is critical that the JSON is well-formed and passes a linter.

### **Evaluation Criteria & JSON Output Structure:**

Produce a only a valid JSON object with the following structure (make sure it is valid and can be read by a JSON parser). Be very transparent about how the `overall_compliance_score` is calculated, detailing the score and weight for each criterion.

**JSON Output Schema:**

```json
{
  "product_category": "string (The primary inferred GCP product category from the table below that this sample is demonstrating)",
  "product_name": "string (The primary inferred GCP product from the table below that this sample is demonstrating)",
  "overall_compliance_score": "integer (0-100)",
  "criteria_breakdown": [
    {
      "criterion_name": "string (A specific, computer-friendly name for the criterion. MUST be one of: 'runnability_and_configuration', 'api_effectiveness_and_correctness', 'comments_and_code_clarity', 'formatting_and_consistency', 'language_best_practices', 'llm_training_fitness_and_explicitness')",
      "score": "integer (0-100 for this specific criterion)",
      "weight": "float (The weight of this criterion in the overall score)",
      "assessment": "string (Your detailed assessment for this criterion, explaining the score given. Be specific.)",
      "recommendations_for_llm_fix": [
        "string (Specific, actionable instructions an LLM could use to directly modify the code. **Recommendations should make the minimum change necessary to correct the identified issue.** For example, if a property has a typo, the recommendation should only correct the typo, not alter other parts of the property name.)"
      ],
      "generic_problem_categories": [
        "string (Keywords or phrases categorizing the types of issues found, e.g., 'API Misuse', 'Readability', 'Configuration Error', 'Missing Comment', 'Style Inconsistency', 'Non-Idiomatic Code'. Aim for a consistent set of categories.)"
      ]
    }
    // ... one object for each criterion below
  ],
  "llm_fix_summary_for_code_generation": [
    "string (A list of all 'recommendations_for_llm_fix' from the breakdowns, suitable for a separate LLM to execute the code changes.)"
  ],
  "identified_generic_problem_categories": [
    "string (A unique list of all 'generic_problem_categories' identified across all criteria.)"
  ]
}
```

### **Detailed Evaluation Criteria and Weights:**

1.  **Runnability & Configuration (criterion\_name: `runnability_and_configuration`, Weight: 0.15)**

      * Is the code sample runnable by default?
      * **You MUST assume that declared dependencies (e.g., in `require`, `import`, `pom.xml`, `go.mod`) are valid and exist in their respective public package managers (NPM, PyPI, Maven Central, etc.). Do not penalize a sample for using a library you do not recognize.** A dependency should only be flagged as non-existent if your knowledge base is highly confident it is a typo or a private, inaccessible package.
      * Does it use minimum parameters, reserving them for environmental configuration (e.g., Cloud Project ID, Cloud Region/Location) and unique resource IDs and relying on hardcoded, literal values in other cases? Does it correctly and clearly use environment variables for necessary system parameters (e.g., GCP project ID, region/location) if applicable?
      * Are all prerequisite configurations or variable settings (like project ID, location) clearly indicated or handled?
      * Assessment should note any assumptions made about the execution environment. There is no need to worry about GCP authentication to run the sample - it can be assumed and not noted.
      * (Note: The experience for documentation readers should only consider code between commented out '[START' and '[END' markers. Code outside these comments supports maintenance and runnability but is not shown to readers.)

2.  **API Effectiveness & Correctness (criterion\_name: `api_effectiveness_and_correctness`, Weight: 0.40)**

      * **Mandatory API Verification:** Before any other assessment, you MUST verify that the primary client library (e.g., `google-cloud-modelarmor`) is publicly available by searching for its official documentation or public package page. All methods, parameters, and properties must then be validated against that official source. **Incorrectly flagging a real, public library as non-existent is a critical failure of this entire task and will invalidate your assessment.**
      * **Prioritize Official Documentation as the Single Source of Truth:** Your validation MUST be based on the latest documentation from official sources (e.g., cloud.google.com, PyPI, Maven Central, npmjs.com). If you find conflicting information in blog posts, forums (like Stack Overflow), or older tutorials, the **official documentation MUST take precedence.**
      * **Recognize Multiple Valid API Patterns:** Be aware that a single API goal (e.g., setting a storage class) may have multiple, equally valid calling patterns in the official documentation. This often includes explicit long-form parameters (e.g., `storageClass: 'VALUE'`) as well as idiomatic language shortcuts (e.g., `{ value: true }`). Do not flag a documented shortcut as an error simply because a more explicit alternative exists.
      * **Distinguish Between Non-Existent and Misused APIs:** When identifying an API misuse, be precise. If a method or property is used incorrectly, first determine if it exists at all.
          * If it **does not exist**, state that it is a hallucinated or non-existent API.
          * If it **is a real API feature but used for the wrong purpose**, state that clearly. Explain the feature's *actual* purpose and why it's a mismatch for the code's intended goal. For example, do not claim a field 'does not exist' when it is actually a real field being used improperly (e.g., using `version_destroy_ttl` instead of `ttl` in Secret Manager).
      * **Do not guess about API correctness.** A hallucinated method (claiming an API exists when it doesn't) or a **false negative (claiming a real, documented API parameter or pattern is invalid, such as with `enable_object_retention` or the `[storageClass]: true` pattern)** are both critical failures that must be heavily penalized under this criterion. For example the ModelArmorClient and @google-cloud/modelarmor are new and are documented in the official Google documentation and at https://github.com/googleapis/googleapis/tree/master/google/cloud/modelarmor/v1 so if you believe it doesn't, please check Google official documentation and https://github.com/googleapis/googleapis first to confirm your belief on libraries, methods, and parameters.
      * Verify the response object structure. Ensure the code correctly handles the actual structure of API responses, rather than an assumed or hallucinated structure.
      * Are the most important and relevant parameters for the demonstrated API call being used correctly and clearly?
      * Does it showcase best practices for interacting with this specific API? (e.g., error handling, resource management if applicable within a small sample).
      * Are essential variables like project ID and location correctly passed to API clients or methods if required by the specific API service?
      * Does the sample implement rudimentary error handling (there is no need to handle specific API errors, just rudimentary error handling)

3.  **Comments & Code Clarity (criterion\_name: `comments_and_code_clarity`, Weight: 0.10)**

      * Are comments helpful and explanatory without being overly verbose or redundant?
      * Do comments clarify the "why" behind non-obvious code sections?
      * Is the code itself clear, readable, and easy to understand for its intended purpose (documentation sample)?

4.  **Formatting & Consistency (criterion\_name: `formatting_and_consistency`, Weight: 0.10)**

      * Is the code formatting consistent *within* the provided sample?
      * **Does it adhere to generally accepted formatting conventions for {{LANGUAGE}}? Adhere to the specific style guides below:**
          * **C\#**: Microsoft's C\# Coding Conventions.
          * **C++**: A well-established style guide such as the Google C++ Style Guide.
          * **Go**: The standard `gofmt` formatting.
          * **Java**: A common style guide such as the Google Java Style Guide.
          * **Javascript**: Common style guides (e.g., consistent with Prettier or a standard ESLint configuration).
          * **PHP**: A common standard such as PSR-12.
          * **Python**: PEP 8 (line length issues can be ignored).
          * **Ruby**: A common community standard such as The Ruby Style Guide.
          * **Rust**: The standard `rustfmt` formatting.
          * **Terraform**: The standard `terraform fmt` formatting.
      * (Note: You can only assess internal consistency. Do not bother stating that cross-sample consistency cannot be fully judged from a single sample).
      * Ignore any errors related to copyright year

5.  **Language Best Practices (criterion\_name: `language_best_practices`, Weight: 0.15)**

      * Does the code follow generally accepted best practices for **{{LANGUAGE}}** (e.g., idiomatic constructs, proper variable naming, efficient use of language features, appropriate rudimentary error handling patterns for the language)?
      * Avoidance of anti-patterns or deprecated features for the given language.
      * Avoid language practices that have not been available for at least two language feature releases, so that samples remain compatible with the previous language version.
      * Prefer using libraries bundled in the language the standard library over other open source dependencies. If using open source dependencies, prefer those that have been released within 1 year and are known to be secure.

6.  **LLM Training Fitness & Explicitness (criterion\_name: `llm_training_fitness_and_explicitness`, Weight: 0.10)**

      * Is the code explicit and self-documenting? It should avoid "magic values" (unexplained literals) and favor descriptive variable names over short, generic ones (e.g., `secretId` instead of `id`).
      * Does the sample use type hints (e.g., in Python, TypeScript) or explicit type declarations (e.g., in Java, C\#) where idiomatic, to reduce ambiguity for both human readers and automated tools?
      * Is the demonstrated pattern clear and unambiguous, providing a strong, positive example for an LLM to learn from? The code's intent should be obvious from reading the code itself.

### Instructions for the AI Reviewer:

  * **Mandatory Verification Workflow:** Your first and most critical step for every sample is to verify the primary client library.
    1.  **Identify:** Find the main `import` or `require` statement (e.g., `from google.cloud import modelarmor_v1`).
    2.  **Search:** Use your search tools to find its official documentation page on `cloud.google.com` or its package page on a public repository (PyPI, npm, etc.).
    3.  **Proceed:** Only after confirming the library is public and real should you proceed with the rest of the evaluation. If you cannot find it after a diligent search, you may then conclude it is non-public or non-existent.
  * **Generate Minimal and Precise Fixes:** When creating a `recommendation_for_llm_fix`, ensure it is the most direct and minimal change required. Before finalizing your recommendation, mentally double-check that your suggested fix precisely corrects ONLY the identified issue and does not inadvertently alter other correct parts of the code.
  * **Avoid Double Penalties:** Each distinct problem in the code should be penalized only once, under the most specific and relevant criterion. For example, if an API call is missing error handling, the score deduction and the recommendation must fall under `api_effectiveness_and_correctness`, not under `language_best_practices`.
  * You must validate that the code correctly interacts with the structure of API response objects.
  * Be critical but constructive.
  * The `recommendations_for_llm_fix` should be precise enough that another LLM could attempt to apply them directly to the code.
  * Ensure the `overall_compliance_score` is a weighted average of the individual criterion scores.